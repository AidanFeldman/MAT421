{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3uS2UpLFl5h75sdTi6T/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AidanFeldman/MAT421/blob/main/Section_1.1_1.2_1.3_1.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Linear algebra is a field of study that can be used for data science and machine learning."
      ],
      "metadata": {
        "id": "vnjAa1bA3smt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elements of Linear Algebra\n",
        "\n",
        "**Linear subspace** is the set that can be reached through a combination of scalar multiplication and addition of vectors. The zero vector is in all linear subspaces. The **span** of a set of vectors is the linear subsapce.\n",
        "\n",
        "\n",
        "The **column space** of a matrix is the span of the columns of that matrix.\n",
        "\n",
        "A set of vectors are considered **linearly independent** if there is no combination of scalars such that the sum of the scaled vectors is the zero vector (with at least one non-zero scalar).\n",
        "\n",
        "The **basis** of a subspace is a set of vectors that are linearly independent and span all of the subspace.\n",
        "\n",
        "# Orthogonality\n",
        "\n",
        "A set of vectors are **orthonormal** if they sum to the zero vector and one of them has a magnitude of 1.\n",
        "\n",
        "If there is an optimal combination, but that combination is for some reason impossible, matrix **best approximation** can be used to find the solution closest to that optimal combination of variables that follows all practical constraints.\n",
        "\n",
        "**Eigenvalues**\n",
        "\n",
        "The **eigenvalue** of a square matrix is a constant such that the multiplication of the square matrix by its *eigenvector* is equal to the product of the eigenvalue by the eigenvector."
      ],
      "metadata": {
        "id": "ddXu_qS84ClI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "**Linear regression** approximates the relationship between two variables with a simple linear equation. This is a simple but often inadequate method to represent this relationship.\n",
        "\n",
        "The **QR decomposition** is a tool that can be used to solve the least squares problem. For a given matrix, A, its QR decompisition is such that A=QR where Q is a matrix with the same dimensions as A and R is a suqare matrix with its dimensions determined by the number of columns in A.\n",
        "\n",
        "Suppose that A is a matrix and b is a vector with as many rows as A. To solve the system Ax=b, the **Least-squares Problems** is satisfied such that the best approximation of (Ax-b) is solved.\n",
        "\n",
        "The common way to solve for a linear regression is, for a set of points (x,y), to minimize the sum of the squares of the difference between every projected y and actual y for an equation where the projected y is represented as the sum of some constant, b, and the product of some constant, a, and the given x variable."
      ],
      "metadata": {
        "id": "wJ4ZilvR87h0"
      }
    }
  ]
}